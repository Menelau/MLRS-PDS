{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5gtydFXVDaF","executionInfo":{"status":"ok","timestamp":1649771238557,"user_tz":240,"elapsed":18713,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"4f46aa00-726e-486b-eb8f-415dbf508dc4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/lit/lit_lit/')"],"metadata":{"id":"H1qS6yapU7hw","executionInfo":{"status":"ok","timestamp":1649771238559,"user_tz":240,"elapsed":16,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xWSz33daXFF2","executionInfo":{"status":"ok","timestamp":1649771244562,"user_tz":240,"elapsed":6016,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","#from matplotlib.mlab import griddata\n","from scipy.interpolate import griddata\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","import tensorflow as tf\n","from neural_network import *\n","from ensembling_methods import *\n","# from figure_grid import *\n","\n","# Use autograd to get easy manifold tangents; could also do this in Tensorflow but less conveniently.\n","import autograd.numpy as np\n","from autograd.scipy.special import expit\n","from autograd import elementwise_grad"]},{"cell_type":"code","source":["from __future__ import print_function\n","import six\n","import time\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics import roc_auc_score, accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","def l1_loss(x):\n","  return tf.reduce_sum(tf.abs(x))\n","\n","def l2_loss(x):\n","  return tf.nn.l2_loss(x)\n","\n","class cachedproperty(object):\n","  \"\"\"Simplified https://github.com/pydanny/cached-property\"\"\"\n","  def __init__(self, function):\n","    self.__doc__ = getattr(function, '__doc__')\n","    self.function = function\n","\n","  def __get__(self, instance, klass):\n","    if instance is None: return self\n","    value = instance.__dict__[self.function.__name__] = self.function(instance)\n","    return value\n","\n","def isint(x):\n","  return isinstance(x, (int, np.int32, np.int64))\n","\n","def onehot(Y, K=None):\n","  if K is None:\n","    K = np.unique(Y)\n","  elif isint(K):\n","    K = list(range(K))\n","  data = np.array([[y == k for k in K] for y in Y]).astype(int)\n","  return data\n","\n","def minibatch_indexes(lenX, batch_size=256, num_epochs=50, **kw):\n","  n = int(np.ceil(lenX / batch_size))\n","  for epoch in range(num_epochs):\n","    for batch in range(n):\n","      i = epoch*n + batch\n","      \n","      yield i, epoch, slice((i%n)*batch_size, ((i%n)+1)*batch_size)\n","\n","def train_feed(idx, models, **kw):\n","  \"\"\"Convert a set of models, a set of indexes, and numpy arrays given by the\n","  keyword arguments to a set of feed dictionaries for each model.\"\"\"\n","  feed = {}\n","  for m in models:\n","   \n","    feed[m.is_train] = True\n","    for dictionary in [kw, kw.get('feed_dict', {})]:\n","      for key, val in six.iteritems(dictionary):\n","        attr = getattr(m, key) if isinstance(key, str) and hasattr(m, key) else key\n","        \n","        if type(attr) == type(m.X):\n","          if len(attr.shape) >= 1:\n","            if attr.shape[0] is None:\n","              feed[attr] = val[idx]\n","  \n","  return feed\n","\n","def train_batches(models, X, y, **kw):\n","  for i, epoch, idx in minibatch_indexes(len(X), **kw):\n","    yield i, epoch, train_feed(idx, models, X=X, y=y, **kw)\n","\n","def reinitialize_variables(sess):\n","  \"\"\"Construct a Tensorflow operation to initialize any variables in its graph\n","  which are not already initialized.\"\"\"\n","  uninitialized_vars = []\n","  for var in tf.compat.v1.global_variables():\n","    try:\n","      sess.run(var)\n","    except tf.errors.FailedPreconditionError:\n","      uninitialized_vars.append(var)\n","  return tf.compat.v1.variables_initializer(uninitialized_vars)\n","\n","def minimize(sess, loss_fn, batches, operations={}, learning_rate=0.001, print_every=None, var_list=None, **kw):\n","  \"\"\"Minimize a loss function over the provided batches of data, possibly\n","  printing progress.\"\"\"\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","  train_op = optimizer.minimize(loss_fn, var_list=var_list)\n","  op_keys = sorted(list(operations.keys()))\n","  ops = [train_op] + [operations[k] for k in op_keys]\n","  t = time.time()\n","  sess.run(reinitialize_variables(sess))\n","  for i, epoch, batch in batches:\n","\n","    results = sess.run(ops, feed_dict=batch)\n","    if print_every and i % print_every == 0:\n","      s = 'Batch {}, epoch {}, time {:.1f}s'.format(i, epoch, time.time() - t)\n","      for j,k in enumerate(op_keys, 1):\n","        s += ', {} {:.4f}'.format(k, results[j])\n","      print(s)\n","\n","def tt_split(X, y, test_size=0.2):\n","  return train_test_split(X, y, test_size=test_size, stratify=y)\n","\n","def elemwise_sq_cos_sim(v, w, eps=1e-8):\n","  assert(len(v.shape) == 2)\n","  assert(len(w.shape) == 2)\n","  num = np.sum(v*w, axis=1)**2\n","  den = np.sum(v*v, axis=1) * np.sum(w*w, axis=1)\n","  return num / (den + eps)\n","\n","def yules_q_statistic(e1, e2, y_test):\n","  n = len(y_test)\n","  n00 = len(e1.intersection(e2))\n","  n01 = len(e1.difference(e2))\n","  n10 = len(e2.difference(e1))\n","  n11 = n - len(e1.union(e2))\n","  assert(n00+n01+n10+n11 == n)\n","  numer = n11*n00 - n01*n10\n","  denom = n11*n00 + n01*n10\n","  if numer == 0:\n","    return 0\n","  else:\n","    return numer / float(denom)\n","\n","def disagreement_measure(e1, e2, y_test):\n","  n = len(y_test)\n","  n01 = len(e1.difference(e2))\n","  n10 = len(e2.difference(e1))\n","  return (n01 + n10) / n\n","\n","def scoring_fun(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = y_pred\n","    else:\n","      preds = y_pred[:,1]\n","    \n","    return accuracy_fun(y_pred, y_true)\n","    #return roc_auc_score(y_true, preds)\n","  else:\n","    return accuracy_fun(y_pred, y_true)\n","\n","def accuracy_fun(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = (y_pred > 0.5).astype(int)\n","    else:\n","      preds = np.argmax(y_pred, axis=1)\n","    return np.mean(y_true == preds)\n","  else:\n","    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n","\n","def error_masks(y_pred, y_true):\n","  if len(y_true.shape) == 1:\n","    #assert(y_true.max() == 1) # binary\n","    if len(y_pred.shape) == 1:\n","      preds = (y_pred > 0.5).astype(int)\n","    else:\n","      preds = np.argmax(y_pred, axis=1)\n","    return (preds != y_true).astype(int)\n","  else:\n","    return (np.argmax(y_true, axis=1) != np.argmax(y_pred, axis=1)).astype(int)\n","\n"],"metadata":{"id":"b-NQQ54gJQqe","executionInfo":{"status":"ok","timestamp":1649771244808,"user_tz":240,"elapsed":269,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYZ-3XhVXFGB"},"source":["## Initialize network"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TyPbxTIkXFGB","executionInfo":{"status":"ok","timestamp":1649771244810,"user_tz":240,"elapsed":14,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}}},"outputs":[],"source":["class Net(NeuralNetwork): \n","  @property\n","  def x_shape(self): return [None, 24]\n","  @property\n","  def y_shape(self): return [None, 2]  \n","\n","  def rebuild_model(self, X, **_):\n","    L0 = X\n","    L1 = tf.compat.v1.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.softplus)\n","    L2 = tf.compat.v1.layers.dense(L1, 256, name=self.name+'/L2', activation=tf.nn.softplus)\n","    L3 = tf.compat.v1.layers.dense(L2,  2, name=self.name+'/L3', activation=None)\n","    return [L1, L2, L3]\n"]},{"cell_type":"code","source":["print(tf.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-dPtlNKZXEa","executionInfo":{"status":"ok","timestamp":1649771244812,"user_tz":240,"elapsed":14,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"16cdb676-6445-4cf7-f437-1849b0790ea3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0\n"]}]},{"cell_type":"code","source":["from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import numpy as np\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","dict_results = {}\n","dict_sd_pc = {}\n","for files in glob('//content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    pc = []\n","    sd_pc = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        item_name = items.split(\".\")[0]\n","        print(item_name)\n","        ss = item_name.split(\"_\")[4] + item_name.split(\"_\")[5]\n","        print(ss)\n","        hh = item_name.split(\"_\")[4]\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_val = input_list['FrameStack'][4]\n","        y_val = input_list['FrameStack'][5]\n","\n","        y_shape = max(y_train)+1\n","        class Net(NeuralNetwork): \n","          @property\n","          def x_shape(self): return [None, X_train.shape[1]]\n","          @property\n","          def y_shape(self): return [None, y_shape]\n","          def rebuild_model(self, X, **_):\n","            L0 = X\n","            L1 = tf.compat.v1.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.softplus)\n","            L2 = tf.compat.v1.layers.dense(L1, 256, name=self.name+'/L2', activation=tf.nn.softplus)\n","            L3 = tf.compat.v1.layers.dense(L2, y_shape , name=self.name+'/L3', activation=None)\n","            return [L1, L2, L3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O982kfb0HD-A","executionInfo":{"status":"ok","timestamp":1649771251550,"user_tz":240,"elapsed":6749,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"4b66165c-0dc6-403d-8be4-9f6c5cd53fe5"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":[""]}]},{"cell_type":"code","source":["from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import numpy as np\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","dict_results = {}\n","dict_sd_pc = {}\n","for files in glob('//content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    pc = []\n","    sd_pc = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        item_name = items.split(\".\")[0]\n","        print(item_name)\n","        ss = item_name.split(\"_\")[4] + item_name.split(\"_\")[5]\n","        print(ss)\n","        hh = item_name.split(\"_\")[4]\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_val = input_list['FrameStack'][4]\n","        y_val = input_list['FrameStack'][5]\n","\n","        y_shape = max(y_train)+1\n","        class Net(NeuralNetwork): \n","          @property\n","          def x_shape(self): return [None, X_train.shape[1]]\n","          @property\n","          def y_shape(self): return [None, y_shape]\n","          def rebuild_model(self, X, **_):\n","            L0 = X\n","            L1 = tf.compat.v1.layers.dense(L0, 256, name=self.name+'/L1', activation=tf.nn.softplus)\n","            L2 = tf.compat.v1.layers.dense(L1, 256, name=self.name+'/L2', activation=tf.nn.softplus)\n","            L3 = tf.compat.v1.layers.dense(L2, y_shape , name=self.name+'/L3', activation=None)\n","            return [L1, L2, L3]\n","\n","      \n","        tf.compat.v1.reset_default_graph()\n","\n","        print(\"train restarts\")\n","        random_restart_models = train_restart_models(Net, 100, X_train, y_train, num_epochs=100, print_every=50)\n","\n","        print(\"train input-space LIT\")\n","        input_space_models = train_diverse_models(Net, 100, X_train, y_train, num_epochs=100, print_every=50)\n","        for i, model in enumerate(input_space_models):\n","          print(\"dasff\",type(model))\n","          model.save(\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"+str(i)+\"_\"+ss+\".pkl\")\n","\n","          all_model_probs = np.array([ model.predict_proba(X_test)[:,1] for model in input_space_models ])\n","          average_probs = all_model_probs.mean(axis=0) # average predicted probability\n","          variance_probs = all_model_probs.var(axis=0) # variance = measure of uncertainty\n","          label_predictions = (average_probs >= 0.5).astype(int) # threshold average to get label prediction\n","          accuracy = np.mean(label_predictions == y_test)\n","          print(items)\n","          print(accuracy)\n","          pc.append(accuracy)\n","          dict_results[\"{}\".format(files)] = '{:.4f}'.format(mean(pc))\n","          np.std(pc, dtype=np.float64)\n","          sd_pc.append(np.std(pc, dtype=np.float64))\n","\n","    dict_sd_pc[\"{}\".format(files)] = '{:.4f}'.format(mean(sd_pc))\n","print(dict_results)\n","print(dict_results.values())\n","print(dict_sd_pc.values())\n","# auc = sklearn.metrics.roc_auc_score(average_probs, y_test)"],"metadata":{"id":"5Rz9NrNagGSy","executionInfo":{"status":"ok","timestamp":1649702745985,"user_tz":240,"elapsed":174988,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ad0552c-a408-47ed-9062-bbbdbe0e327b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["pip install deslib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sYfRh_LR-j07","executionInfo":{"status":"ok","timestamp":1649771277884,"user_tz":240,"elapsed":4525,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"f8fe86ca-d314-44a2-c0bb-d9b4408f2bc2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deslib\n","  Downloading DESlib-0.3.5-py3-none-any.whl (158 kB)\n","\u001b[?25l\r\u001b[K     |██                              | 10 kB 17.0 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 30 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 40 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 51 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 61 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 71 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 81 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 92 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 102 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 112 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 122 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 133 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 143 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 153 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 158 kB 10.2 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (1.4.1)\n","Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (1.21.5)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from deslib) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->deslib) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0->deslib) (3.1.0)\n","Installing collected packages: deslib\n","Successfully installed deslib-0.3.5\n"]}]},{"cell_type":"code","source":["import deslib\n","from deslib.dcs import OLA, MLA\n","from deslib.des import METADES, KNORAU, DESMI, DESP\n","from deslib.des.knora_e import KNORAE\n","from deslib.static import Oracle\n","from sklearn.metrics import accuracy_score\n","import glob\n","import numpy as np\n","from numpy import mean\n","import pickle\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","\n","#dynamic_selection_methods\n","dict_final_results = {}\n","dict_standard_deviation = {}\n","for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    standard_deviation = []\n","    knorae_results = []\n","    accuracy = []\n","    pool = []\n","    list_cls = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_dsel = input_list['FrameStack'][4]\n","        y_dsel = input_list['FrameStack'][5]\n","\n","        ###file_name = files.split(\"\\\\\")[1]\n","        # print(file_name)\n","        ###item_name = items.split(\"\\\\\")[2].split(\".\")[0].split(\"_\")[2]\n","        # print(item_name)\n","        ###ss = file_name.split(\"_\")[0]\n","        # print(ss)\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/random_forest.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"Models_FLT_f/{}/{}_{}.pkl\".format(file_name,ss, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        input_space_models = [Net() for _ in range(2)]\n","        for i, model in enumerate(input_space_models):\n","          # print(\"a\")\n","          # print(model)\n","          aaa = model.load(\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"+str(i)+\"_\"+ss+\".pkl\")   #input_space_model0.pkl\n","          \n","          print(model.load)\n","          print(type(model.load))\n","          models = list_cls.append(aaa)\n","          print(models)\n","          x_merge = np.append(X_train, X_dsel, axis=0)\n","          y_merge = np.append(y_train, y_dsel, axis=0)\n","\n","          # pool_classifiers = model.load\n","          pool_classifiers = models\n","          # method = KNORAE\n","          # method = MLA\n","          # method = OLA\n","          # method = METADES\n","          method = KNORAU\n","          # method = DESMI\n","          # method = DESP\n","\n","          ensemble = method(pool_classifiers)\n","          # ensemble.fit(X_train, y_train)\n","          ensemble.fit(x_merge, y_merge)\n","          # Predict new examples:\n","          yh = ensemble.predict(X_test)\n","          acc = accuracy_score(y_test, yh)\n","          print(acc)\n","          knorae_results.append(acc)\n","\n","    dict_final_results[\"{}\".format(files)] = '{:.4f}'.format(mean(knorae_results))\n","    print(dict_final_results)\n","    np.std(knorae_results, dtype=np.float64)\n","    standard_deviation.append(np.std(knorae_results, dtype=np.float64))\n","    dict_standard_deviation[\"{}\".format(files)] = '{:.4f}'.format(mean(standard_deviation))\n","\n","print(dict_final_results.values())\n","print(dict_standard_deviation.values())"],"metadata":{"id":"dJZa7QbmlGzr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649771305319,"user_tz":240,"elapsed":21540,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"b28d1601-c618-4deb-865a-c1d92caa7ca2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["#single best\n","import deslib\n","from deslib.dcs import OLA, MLA\n","from deslib.des import METADES, KNORAU, DESMI, DESP\n","from deslib.des.knora_e import KNORAE\n","from deslib.static import Oracle\n","from sklearn.metrics import accuracy_score\n","import glob\n","import numpy as np\n","from numpy import mean\n","import pickle\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","from deslib.static.single_best import SingleBest\n","\n","#dynamic_selection_methods\n","dict_final_results = {}\n","dict_standard_deviation = {}\n","dict_sb_results = {}\n","dict_standard_deviation = {}\n","\n","for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    standard_deviation = []\n","    knorae_results = []\n","    accuracy = []\n","    pool = []\n","    list_cls = []\n","    single_best_results = []\n","    sd_single_best = []\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_dsel = input_list['FrameStack'][4]\n","        y_dsel = input_list['FrameStack'][5]\n","\n","        ###file_name = files.split(\"\\\\\")[1]\n","        # print(file_name)\n","        ###item_name = items.split(\"\\\\\")[2].split(\".\")[0].split(\"_\")[2]\n","        # print(item_name)\n","        ###ss = file_name.split(\"_\")[0]\n","        # print(ss)\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/random_forest.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"Models_FLT_f/{}/{}_{}.pkl\".format(file_name,ss, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        input_space_models = [Net() for _ in range(2)]\n","        for i, model in enumerate(input_space_models):\n","          # print(\"a\")\n","          # print(model)\n","          aaa = model.load(\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"+str(i)+\"_\"+ss+\".pkl\")   #input_space_model0.pkl\n","          \n","          print(model.load)\n","          print(type(model.load))\n","          models = list_cls.append(aaa)\n","          print(models)\n","          x_merge = np.append(X_train, X_dsel, axis=0)\n","          y_merge = np.append(y_train, y_dsel, axis=0)\n","          pool_classifiers = models\n","          sb_bdt = SingleBest(pool_classifiers = pool_classifiers, scoring = None, random_state = None, n_jobs = -1)\n","          sb_bdt.fit(X_dsel, y_dsel)\n","          sb_bdt.predict(X_dsel)\n","          acc = sb_bdt.score(X_test, y_test, sample_weight=None)\n","          single_best_results.append(acc)\n","\n","    dict_sb_results[\"{}\".format(files)] = '{:.4f}'.format(mean(single_best_results))\n","    print(dict_sb_results)\n","    np.std(single_best_results, dtype=np.float64)\n","    sd_single_best.append(np.std(single_best_results, dtype=np.float64))\n","    dict_standard_deviation[\"{}\".format(files)] = '{:.4f}'.format(mean(sd_single_best))\n","    print(dict_standard_deviation)\n","\n","print(dict_sb_results.values())\n","print(dict_standard_deviation.values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EMH04vG6irKs","executionInfo":{"status":"ok","timestamp":1649714674272,"user_tz":240,"elapsed":118753,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"c3d4cec5-4c30-4166-a766-897c16136fda"},"execution_count":395,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]},{"cell_type":"code","source":["#oracle\n","import deslib\n","from deslib.dcs import OLA, MLA\n","from deslib.des import METADES, KNORAU, DESMI, DESP\n","from deslib.des.knora_e import KNORAE\n","from deslib.static import Oracle\n","from sklearn.metrics import accuracy_score\n","import glob\n","import numpy as np\n","from numpy import mean\n","import pickle\n","from glob import glob\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import auc\n","import sklearn.metrics as metrics\n","from sklearn import metrics\n","from sklearn.metrics import fbeta_score, make_scorer\n","import sklearn\n","from numpy import mean\n","from deslib.static import Oracle\n","\n","dict_final_results = {}\n","dict_standard_deviation = {}\n","dict_sb_results = {}\n","dict_standard_deviation = {}\n","dict_final_results_oracle = {}\n","dict_standard_deviation = {}\n","\n","for files in glob('/content/drive/MyDrive/lit/lit_lit/reserve/processed_8/*'):\n","    print(files)\n","    standard_deviation = []\n","    knorae_results = []\n","    accuracy = []\n","    pool = []\n","    list_cls = []\n","    single_best_results = []\n","    sd_single_best = []\n","    standard_deviation = []\n","    oracle_results = []\n","    sd_oracle = []\n","\n","    for items in glob(\"{}/*.*\".format(files)):\n","        print(items)\n","        data = np.load(items, allow_pickle=True)\n","        input_list = data.tolist()  # How to get x_train\n","        X_train = input_list['FrameStack'][0]\n","        X_test = input_list['FrameStack'][1]\n","        y_train = input_list['FrameStack'][2]\n","        y_test = input_list['FrameStack'][3]\n","        X_dsel = input_list['FrameStack'][4]\n","        y_dsel = input_list['FrameStack'][5]\n","\n","        ###file_name = files.split(\"\\\\\")[1]\n","        # print(file_name)\n","        ###item_name = items.split(\"\\\\\")[2].split(\".\")[0].split(\"_\")[2]\n","        # print(item_name)\n","        ###ss = file_name.split(\"_\")[0]\n","        # print(ss)\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/boosting_with_perceptron.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/random_forest.pkl\".format(file_name, item_name))[0]\n","        # model_load = glob.glob(\"Models_FLT_f/{}/{}_{}.pkl\".format(file_name,ss, item_name))[0]\n","        # model_load = glob.glob(\"models_f/{}/split_{}/bagging_with_decision_tree.pkl\".format(file_name, item_name))[0]\n","        input_space_models = [Net() for _ in range(2)]\n","        for i, model in enumerate(input_space_models):\n","          # print(\"a\")\n","          # print(model)\n","          aaa = model.load(\"/content/drive/MyDrive/lit/lit_lit/input_space_model\"+str(i)+\"_\"+ss+\".pkl\")   #input_space_model0.pkl\n","          \n","          print(model.load)\n","          print(type(model.load))\n","          models = list_cls.append(aaa)\n","          # print(models)\n","          x_merge = np.append(X_train, X_dsel, axis=0)\n","          y_merge = np.append(y_train, y_dsel, axis=0)\n","          pool_classifiers = models\n","          oracleeeee = deslib.static.oracle.Oracle(pool_classifiers=models, random_state=None, n_jobs=-1)\n","\n","          oracleeeee.fit(X_dsel, y_dsel)\n","          y_pred = oracleeeee.predict(X_test, y_test)\n","          accuracyyy = accuracy_score(y_test, y_pred)\n","          print(accuracyyy)\n","          oracle_results.append(accuracyyy)\n","\n","\n","    dict_sb_results[\"{}\".format(files)] = '{:.4f}'.format(mean(oracle_results))\n","    print(dict_sb_results)\n","    np.std(oracle_results, dtype=np.float64)\n","    sd_single_best.append(np.std(oracle_results, dtype=np.float64))\n","    dict_standard_deviation[\"{}\".format(files)] = '{:.4f}'.format(mean(sd_single_best))\n","    print(dict_standard_deviation)\n","\n","print(dict_sb_results.values())\n","print(dict_standard_deviation.values())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_B-IT_OPLVZ","executionInfo":{"status":"ok","timestamp":1649714854021,"user_tz":240,"elapsed":119476,"user":{"displayName":"Enoch Samuel","userId":"16090837763996882893"}},"outputId":"9aecca32-12ff-41ff-80c6-542f5e3cd482"},"execution_count":396,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"2D-Manifold.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}
